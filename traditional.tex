In this section, we describe common approaches that are traditionally
used at HPC sites to install scientific software and to deal with the
growing number of modules over the lifetime of an HPC system. We start
our discussion by module tools used by many sites, followed by a brief
overview of the concept of module files and various module naming
schemes which are in use today.  Next, we review commonly used
workflows for installing scientific software. Finally,
we highlight the lack of collaboration between HPC sites and the absence
of tools for dealing with the problems mentioned along the way.

\subsection{Environment modules system}
\label{sec:env_modules_system}

For software not installed in standard system locations a user's
\texttt{\small\$PATH} must be modified to include the install location of a
software package.  Other enviroment variables may also be required for the
software to work, e.g., \texttt{\small\$LD\_LIBRARY\_PATH}, etc.
One approach is to provide a shell script for each software package which
users can source to modify their working environment.

The environment modules system provides a technique with several major
advantages. The first is that a single file provides all settings
required for a user to access the package.  There is no need for a
separate file for each shell. Different implementations of an
environment modules system are available which all share a similar
user interface, typically consisting of a command named \texttt{\small module}:
{\small
\begin{alltt}
    \textbf{\% module} \emph{[options]} \emph{<subcmd>} ...
\end{alltt}
}
\noindent
Here, \emph{\texttt{\small<subcmd>}} can be \texttt{\small load} to load modules,
\texttt{\small unload} to unload modules, \texttt{\small list} to list the loaded
modules, or \texttt{\small avail} to print the modules that can be currently
loaded, among others.

To access a particular software package named `\emph{foo}', the user simply
\emph{loads} the matching module:
{\small
\begin{alltt}
    \textbf{\% module load \emph{foo}}
\end{alltt}
}
\noindent
The second major advantage is that a user can \emph{unload} a previously
loaded module, to revert changes in the environment and restore their
environment to the one they had before they loaded that module.  This means
that users can control their working environment by switching between
versions of the same package or changing between different compilers and
MPI stacks.  These power features show why the environment module
system has widely adopted on HPC systems since the late 1990s.


%\kenneth{we also need to briefly describe \texttt{module avail} and
%\texttt{module list}, since they're used in examples; maybe we also need to briefly
%mention that the user interacts with module files via the \texttt{module} command,
%which accepts various subcommands}
%

In the traditional implementations, the \texttt{\small module} command
is implemented as a simple shell function (for Bourne-compatible shells) or
alias (for csh-compatible shells) which evaluates the commands printed by a
helper tool (e.g., \texttt{\small modulecmd}) to standard output. This helper tool
implements the actual functionality of identifying the specified subcommand,
locating and parsing the corresponding module files, and generating the
commands necessary to modify the user's environment.

Over time, multiple implementations of the helper tool
have been developed. The original implementation~\cite{furlani91} was a collection
of shell scripts. Today, the most commonly used version is written in C, using
the Tcl scripting language to parse and evaluate module files~\cite{em}. A second
implementation, which was never packaged as a release and is still marked as
experimental by the authors, is implemented using only Tcl~\cite{em}. For the most
part, these two implementations
offer identical functionality, however they suffer from the usual
problem of keeping different code bases aligned, which sometimes leads
to surprises when switching between them due to subtle differences. In addition,
there exists a
fork of the Tcl-only implementation which has been heavily adjusted to meet
the requirements of the DEISA (Distributed European Infrastructure for
Supercomputing Applications) project~\cite{wikiDEISA}.  In
1997, a C-only implementation was made
available named \emph{cmod}~\cite{cmod}., but this has not been updated since 1998.


%\remark{briefly explain Tcl/C and Tcl-only environment module tools}

While these implementations provide the desired basic functionality, they
vary in how well maintained they are. In all cases, development
progresses slowly. For example, there has been no activity in the
(publicly accessible) version control system of the Tcl-only
implementation for about two years. That is, new features such as
improved support for working with modules organized in a hierarchical way are
unlikely to happen any time soon. In the case of the Tcl/C modules
there are active discussions on the modules-interest mailing list, but
changes or improvements are infrequent as well. At the time of writing, the
latest available version (3.2.10) was released Dec.~2012, which was over one year
after the previous release.

In 2009, a new implementation of the module system called
Lmod~\cite{taccLmod} has
been made available. It is an entirely new modules tool implemented in
Lua, providing functionality that is (mostly) compatible with the Tcl-based
implementations.  Lmod is actively developed and maintained, and has
an active and thriving community. We discuss Lmod in detail in
Section~\ref{sec:lmod}.

%\markus{From what I can see, there are at least . However, the answers from the maintainers
%suggest to me that they have no interest in changing things too much. What is
%your impression? I'm also not sure whether we should really write this down
%in a paper ;-)}

%\markus{One possible reviewer comment may be: Why didn't you contribute a patch to
%the existing project rather than reinventing the wheel? Do we have a good
%answer to this? Robert, did you actually try to contribute and they rejected
%to include the patch?}
%
%\robert{The truth was that I saw the same behaviour in
%  environment modules mailing list.  My extreme dislike of Tcl and
%  complete lack of understanding of the internals of Tcl/C modules
%  meant that I wasn't going to patch Env. Modules.   I really didn't
%  plan to take over the Env. Modules world.  I thought I'd prototype
%  this new module system, then someone who understood Tcl/C and
%  modules would convert the prototype into Tcl/C.}
%
%\robert{But it was very clear from the beginning that supporting
%  the hierarchy was going to require a major refactoring.  There has
%  to be a notion of inactive modules.  Lmod uses a table that it
%  encodes in the enviroment.  This made it easier to support inactive
%  modules, properties.}

\subsection{Module files}
\label{sec:Module_files}

In essence, module files are shell-independent textual description of
what needs to be changed in the user's environment to make a
particular software package available. Such changes may include the
adjustment of common environment variables such as \texttt{\small \$PATH},
\texttt{\small \$CPATH} and \texttt{\small \$LIBRARY\_PATH}, respectively pointing
to the locations for binaries, header files and libraries, or setting additional
package-specific variables. In addition,
module files typically include a brief one-line description of the
package displayed by \texttt{\small module whatis}, as well as a longer help
text printed by \texttt{\small module help} to describe the basic usage,
where to find the package documentation, and whom to contact in case
of usage problems.

Module files are searched for in directories specified by the
environment variable \texttt{\small \$MODULEPATH}. The name of a module is defined
as the path to the corresponding module file in one of the directories that are
part of \texttt{\small \$MODULEPATH}. For example, the module file located in
\texttt{\small \emph{<prefix>}/GCC/4.8.2}
provides a module for version 4.8.2 of the GNU Compiler Collection (GCC) with the
name \texttt{\small GCC/4.8.2}, with \texttt{\small\emph{<prefix>}} being one of the
\texttt{\small \$MODULEPATH} entries.

\subsection{Module naming scheme}
\label{sec:Module_naming_scheme}
When the environment module system was invented HPC sites typically had
one single compiler available, i.e. the system compiler.
There weren't even multiple versions of the system compiler installed
at the same time.  Today, HPC sites have multiple compilers (GCC, Intel,
Clang, PGI, \ldots) available, typically providing different versions for
each of them.

While there is some interopability for pure C programs across
compilers, there is none for Fortran and C++ programs.  This implicates that
also multiple builds of these libraries (e.g., Boost) need to be provided,
one for each compiler (version).
Moreover, since packages such as MPI implementations are inherently tied to a
particular compiler and most often even a particular version, disambiguating
module names can be a daunting task. For example, modules corresponding to
version~1.7.3 of the OpenMPI library built with different compilers can be named
as follows:
{\small
\begin{alltt}
    \textbf{\% module avail OpenMPI}
    OpenMPI/1.7.3-GCC-4.8.2
    OpenMPI/1.7.3-Intel-14.0
\end{alltt}
}

The situation becomes even more complicated for scientific software
packages like WRF compiled with a particular compiler and linked
against a particular MPI library, e.g.:
{\small
\begin{alltt}
    \textbf{\% module avail WRF}
    WRF/3.5-GCC-4.8.2-OpenMPI-1.7.3
    WRF/3.5-Intel-14.0-OpenMPI-1.7.3
\end{alltt}
}
\noindent
Note that such packages in many cases also depend on a set of mathematical
libraries, such as OpenBLAS+(Sca)LAPACK+FFTW vs. ACML vs. Intel MKL, which may
extend the module name even further.

A common solution to this issue is to define so-called \emph{toolchain}
modules, packaging together a compiler, an MPI library, and one or more packages
providing linear algebra and FFT functionality. For example, a \texttt{\small goolf}
toolchain module may combine (i.e., implicitly load modules for) GCC,
OpenMPI, OpenBLAS, (Sca)LAPACK and FFTW---each with a well-defined version. The
first WRF module as shown above may then simply refer to a toolchain instead of the
individual packages, i.e., \texttt{\small WRF/3.5-goolf-1.6.10}.
The downside of using toolchains however is that users have
to be aware what is hidden behind the (often rather cryptic) toolchain names,
and that a toolchain version has no direct relationship with the versions of
the encapsulated packages.

\remark{the paragraph below needs to be integrated in this text}

One common attempt to better structure the potentially overwhelming set of available
module files is to categorize them in different subdirectories. Additionally,
each of the subdirectories can be listed separately in the
\texttt{\small\$MODULEPATH}, resulting in slightly shorter module names that are
nicely separated in the output of
\texttt{\small module avail}, e.g.:
{\small
\begin{alltt}
    \textbf{\% module avail}
    ----- <\emph{<prefix>}/compiler -----
    GCC/4.8.2   Intel/14.0  Clang/3.4
    -------- \emph{<prefix>}/mpi --------
    OpenMPI/1.7.3-GCC-4.8.2
    OpenMPI/1.7.3-Intel-14.0
    -------- \emph{<prefix>}/apps --------
    WRF/3.5-GCC-4.8.2-OpenMPI-1.7.3
    WRF/3.5-Intel-14.0-OpenMPI-1.7.3
\end{alltt}
}
\noindent
However, this involves picking a single category for each software package to
install, which may become cumbersome with scientific software that crosses multiple
research domains. The toolchain concept also offers the possibility to categorize
modules by toolchain, but if a software package is available for multiple
toolchains, it will show up in multiple sections of the \texttt{\small module
avail} output, which is not desirable.


It should be pointed out that although all of the approaches to name and
categorize module files presented above try to improve the overall
organization of the available modules, a typical module listing on an HPC
system can still be overwhelming, as the total number of modules can easily
be in the order of several hundreds. Moreover, all traditional aproaches offer a
multitude of options for (especially novice) users to shoot themselves in the
foot, that is, to load modules which are incompatible to each other. While
module files offer the possibility to specify conflicts and thereby can prevent that
incompatible modules are loaded together, all conflicting modules have
to be explicitly listed. For example, the OpenMPI module may
specify that it is incompatible with modules for other MPI libraries like
Intel MPI or MVAPICH. However, this means that these conflict specifications have
to be adjusted every time an additional MPI library is installed on the
system, which is clearly a maintenance nightmare from the perspective of the
system administrator.

The `flat' module naming scheme discussed here is common, but it places a significant
burden on users. If a user requires only a single application like WRF, picking a
module is fairly straightforward. But when multiple software packages are required
at the same time, e.g., when an application developer is using multiple libraries to
build a parallel application, he or she is required to pick modules for a
compiler, MPI stack and other required libraries that are compatible with each other.
When a mismatched set of modules is choosen, the developer might get lucky and
see that their application fails to run or die immediately.  However, the
application may also fail in subtle and mysterous ways, and can thus consume a
great deal of system staff time in trying to resolve the failure.
Section~\ref{sec:hierarchical} outlines another approach to remove this burden
from users, using a hierarchical module naming scheme.


%\remark{main issues with module files: maintaining consistency (contents of module files, naming),
%putting the burden on users to correctly align things and not run into trouble (avoiding conflicts, etc.)}
%
%
%\remark{module naming scheme should be discussed separately from module files, different concepts,
%making the distinction is important in the paper context}

%\remark{flat naming scheme (most common?),

\subsection{Building and installing scientific software}
\label{sec:installing}

HPC sites around the world use a wide variety of methods and tools to
install scientific software. In this section, we briefly discuss these while
highlighting their shortcomings and common problems. These observations are
supported by the results of a recent poll concerning these
topics~\cite{ISC14bof}.

\subsubsection{Manual installation}

Commonly, sites rely heavily on the manpower of (a part of)
the user support team, and simply manually install software packages following
the install guides that are either composed by themselves over time or are provided
by the respective software development teams (that is, if those are available, and
sufficiently detailed and up-to-date).

\subsubsection{Scripting}

Frequently, sites end up resorting to putting together a collection of (most
commonly bash) scripts to automate the often repititive and error-prone tasks of
configuring, building and installing the software packages, in whichever scripting
language is of preference at that time. Typically, this quickly results in a pile of
loosely coupled hard-to-maintain scripts, which are more often than not only really
understood by just a small fraction or even a single member of the user support
teams~\cite{Jim}, even though they are (heavily) relied upon. On top of this, these
scripts tend to have the site software installation policies (which are likely quite
site-specific) hardcoded into them, leaving little flexibility for other HPC
sites using a slightly different policy to reuse them as is (assuming the scripts
are made available to others).

\subsubsection{Package managers}

Yet another approach is to rely on the package managing tools used
by the operating system, e.g. RPMs and \texttt{\small yum} for RedHat-based systems,
\texttt{\small apt-get} and Debian packages for Debian-like systems, Portage for
Gentoo, etc.
Package managers have adequate solutions for some aspects of
installing large software stacks, including dependency tracking/resolution, software
updates, uninstalling software, etc. However, they are ill-suited for dealing with
certain peculiarities that come into play when installing scientific software on
HPC systems, such as requiring multiple builds/versions of the same software package
to be installed at the same time and heavily customized install procedures involving
non-standard tools, as opposed to the \texttt{\small configure} --
\texttt{\small make} -- \texttt{\small make install} paradigm commonly used by
system software). Also, the package specification formats (e.g.,
\texttt{\small .spec files for RPMs}) tend to have little support for factoring out
common patterns in install procedures, resulting in lots of copy-pasting and thus
become hard to maintain.

Nevertheless, a couple of the larger HPC sites in the world are taking this
packaging approach, since they are able to dedicate large amounts of manpower to
the task of getting scientific software installed; this is typically infeasible for
smaller HPC sites however. Besides these concerns, the efforts spent on
shoe-horning the install procedures of scientific software into the package
specifications are unlikely to
benefit other HPC sites due to, again, little control to apply their own
site-specific installation policies without spending significant additional time to
modify the package specifications to their needs. Examples include
LosF~\cite{lmodSC11} developed by the Texas Advanced Computing Centre (TACC) to
leverage in-house built RPMs (which provide both the software itself
and the accompanying module file), and the XSEDE Compatible Basic Cluster
(XCBC)~\cite{Fischer14} approach which basically consists of a collection of RPMs that
allow for making an HPC system `XSEDE-compatible'.

\subsubsection{Custom tools}

Other solutions include tools custom-made for installing scientific software
on HPC systems. We briefly discuss a number of these in
Section~\ref{sec:related_work}. Typically, these tools were developed in-house for
a certain period of time, after having started as yet another bunch of
scripts being hacked together, up to the point where they were deemed potentially
useful for other HPC sites as well. Unfortunately, these projects typically die a
silent death as quickly as they surfaced, due to basically being a one-man project,
lack of documentation and wide-spread adoption by different HPC sites, inadequate
flexibility and features, etc. We discuss one notable
exception, \easybuild{}~\cite{EasyBuildSC12}, in detail in
Section~\ref{sec:easybuild}.

\subsubsection{Creating module files}

Usually, the creation of module files remains to be handled manually since the
procedure is often deemed ``simple enough". However, this significantly impedes
maintaining a consistent set of module files, and again is likely to result in
imposing the responsibility of creating module files for software installations
on one person or, at best, a handful of people. This is obviously a major concern
on production systems w.r.t.\ ensuring continuity of support for installing
scientific software for end users.

%%???
%%
%%\remark{manual, in-house scripting, 'Jim', little to no collaboration across
%%HPC sites (a few exceptions!)}
%%
%%\markus{Does it make sense to also mention RPM and DEB packages? For a
%%regular software install, the downside is that only one version can be
%%available. But of course packaging systems can be combined with our proposed
%%approach to roll out the software on a bunch of nodes (see TACC).}
%%
%%\kenneth{imho, yes, it makes sense to mention the open issues with traditional
%%packaging systems: only one version/build per software package, usually little
%%support for the mess you run into with scientific software, ...}
%%
%%\remark{issues with using packaging tools like RPM (cfr. TACC): requires tons of manpower,
%%very little flexibility for site-specific modifications, little collaboration between sites thus
%%lots of duplicate effort}

\subsection{Lack of collaboration, tools and policies}
\label{sec:traditional_lack}

Even though the problems w.r.t.\ installing scientific software and using
traditional module naming schemes are well recognized, there is an abundant lack
of available tools and policies that try to provide a solution to these problems.
Additionally, there has been very little collaboration between HPC sites on these
issues, despite them being a significant burden for HPC user support teams
(especially the smaller ones). Even though HPC sites around the world are facing
very similar problems in this area, lots of duplicate effort is still being done,
resulting in a tremendous waste of manpower (and hence time and money), and a loss
of opportunities to benefit from a collaborative effort, e.g., capturing the
immensely valuable expertise that is available across HPC sites worldwide.

In the remainder of this paper we present a modern alternative approach to
installing scientific software, which involves a different way of organizing
module files and using emerging appropriate tools, with the intent to resolve
these common issues.


%???
%\remark{manual creating of module files (consistency issues), repetitive \&
%error-prone work of installing scientific software, \ldots}
