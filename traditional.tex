In this section, we describe common approaches that are traditionally used at HPC
sites to install and manage scientific software over the lifetime of an HPC system.
We start by describing traditional module tools used by many sites, then introduce
the concept of module files and various module naming schemes in common use today.
Next, we review typical workflows for installing scientific software. Finally,
we highlight the lack of collaboration among HPC sites with regard to these topics.

\subsection{Providing access to software using environment modules}
\label{sec:env_modules_system}

For software not installed in standard system locations, a user's
\texttt{\small\$PATH} environment variable is often modified to include the install
location of the binaries it provides, in order to make them easily accessible.
Other environment variables may also need to be changed:
\texttt{\small\$LD\_LIBRARY\_PATH} for libraries required at runtime,
\texttt{\small\$CPATH} for paths to include directories, etc.
One approach is to provide a shell script for each software package that
users can source to modify their working environment, for each of the supported
shells.

The environment modules system extends this technique by managing this sourcing step ``under the hood.'' This approach has several major
advantages. The first is that all users configure their environment in the same way; there is no need for a
separate approach for each type of shell. There are several popular implementations
all sharing a similar
user interface based on a command named \texttt{\small module}:
{\small
\begin{alltt}
    \textbf{\% module} \emph{[options]} \emph{<subcmd>}\
\end{alltt}
}
\noindent
Typical subcommands (among many) include \texttt{\small load},
\texttt{\small unload}, \texttt{\small list} (to list all loaded
modules), and \texttt{\small avail} (to print the modules that are currently available for loading).

To access a particular software package named `\emph{foo}', the user simply
`loads' the corresponding module file:
{\small
\begin{alltt}
    \textbf{\% module load \emph{foo}}\
\end{alltt}
}

\noindent
The second major advantage is that users can \emph{unload} a previously
loaded module, to undo changes in the environment and restore their
environment to the one they had before they loaded a particular module.  This means
that users maintain control of their working environment as they switch between
different versions of the applications, libraries, compilers, or MPI stacks.  These features explain why the environment module
system has become ubiquitous on HPC systems since the late 1990s.


%\kenneth{we also need to briefly describe \texttt{module avail} and
%\texttt{module list}, since they're used in examples; maybe we also need to briefly
%mention that the user interacts with module files via the \texttt{module} command,
%which accepts various subcommands}
%

In all implementations, the \texttt{\small module} command
is implemented as a simple shell function (for Bourne-compatible shells) or
alias (for csh-compatible shells) which evaluates the commands printed to standard output by a
helper tool (e.g., \texttt{\small modulecmd}). This helper tool
does the heavy lifting: identifying the specified subcommand,
locating and parsing the corresponding module files, and generating the
commands necessary to modify the user's environment.

Over time, multiple implementations of the helper tool
have been developed. The original implementation~\cite{furlani91} was a collection
of shell scripts. Today, the most commonly used version is written in C, using
the Tcl scripting language to parse and evaluate module files~\cite{em}. A second
implementation, never packaged as a release and still marked as
experimental by the authors, is implemented using only Tcl~\cite{em}. For the most
part, these two implementations
offer identical functionality, but subtle differences sometimes leads to surprises when switching between them. A third product,
a
fork of the Tcl-only implementation, has been heavily adjusted to meet
the requirements of the DEISA (Distributed European Infrastructure for
Supercomputing Applications) project~\cite{wikiDEISA}.  In
1997, a C-only implementation named \emph{cmod}~\cite{cmod} made its debut, but this product has not been updated since 1998.


%\remark{briefly explain Tcl/C and Tcl-only environment module tools}

While these implementations provide the desired basic functionality, support has been uneven at best. In all cases, development
progresses slowly. For example, there has been no activity in
(the publicly accessible version of) the Tcl-only
implementation for about two years. It is reasonable to assume, therefore, that new features (e.g., improved support for hierarchical schemes, see below) are
unlikely to happen any time soon. In the case of the Tcl/C implementation
there are active discussions on the modules-interest mailing list, but
changes or improvements are infrequent as well. At the time of writing, the
latest available version (3.2.10) was released Dec.~2012, which was over one year
after the previous release.

In 2009, a radically new implementation of the module system emerged called
Lmod~\cite{taccLmod}. Implemented from the ground up in
Lua, it is largely compatible with the Tcl-based
implementations.  Lmod is actively developed and maintained, and has
an active and thriving community. We discuss Lmod in detail in
Section~\ref{sec:lmod}.

%\markus{From what I can see, there are at least . However, the answers from the maintainers
%suggest to me that they have no interest in changing things too much. What is
%your impression? I'm also not sure whether we should really write this down
%in a paper ;-)}

%\markus{One possible reviewer comment may be: Why didn't you contribute a patch to
%the existing project rather than reinventing the wheel? Do we have a good
%answer to this? Robert, did you actually try to contribute and they rejected
%to include the patch?}
%
%\robert{The truth was that I saw the same behaviour in
%  environment modules mailing list.  My extreme dislike of Tcl and
%  complete lack of understanding of the internals of Tcl/C modules
%  meant that I wasn't going to patch Env. Modules.   I really didn't
%  plan to take over the Env. Modules world.  I thought I'd prototype
%  this new module system, then someone who understood Tcl/C and
%  modules would convert the prototype into Tcl/C.}
%
%\robert{But it was very clear from the beginning that supporting
%  the hierarchy was going to require a major refactoring.  There has
%  to be a notion of inactive modules.  Lmod uses a table that it
%  encodes in the enviroment.  This made it easier to support inactive
%  modules, properties.}

\subsection{Module files}
\label{sec:Module_files}

In essence, module files are shell-independent text descriptions of
the changes to a user's environment required to support a
particular software package. Such changes may include adjusting common environment variables such as \texttt{\small \$PATH},
\texttt{\small \$CPATH} and \texttt{\small \$LIBRARY\_PATH}, respectively pointing
to the locations for binaries, header files and libraries, or setting additional
package-specific variables. In addition,
module files typically include a brief one-line description of the
package displayed by \texttt{\small module whatis}, as well as a longer help
text printed by \texttt{\small module help} to describe basic usage,
where to find the package documentation, and whom to contact in case
of usage problems.

Module files are searched in directories specified by the
environment variable \texttt{\small \$MODULEPATH}. The name of a module is defined
as the path to the corresponding module file in one of the directories that are
part of \texttt{\small \$MODULEPATH}. For example, the module file located in
\texttt{\small \emph{<prefix>}/GCC/4.8.2}
provides a module for version 4.8.2 of the GNU Compiler Collection (GCC) with the
name \texttt{\small GCC/4.8.2}, with \texttt{\small\emph{<prefix>}} being one of the
\texttt{\small \$MODULEPATH} entries.

\subsection{Module naming schemes}
\label{sec:Module_naming_scheme}
When the environment module system was invented, HPC sites typically provided
a single version of a single compiler on each of their systems.
Today, HPC systems provide multiple compilers (GCC, Intel,
Clang, PGI, \ldots), each available in multiple versions.

In most cases (there are some exceptions for programs written in pure C), programs
compiled with one version of a compiler cannot safely link to libraries compiled
with another. This means that multiple builds of libraries (e.g., Boost) need to be
provided, one for each compiler and version.
Moreover, since packages such as MPI implementations are inherently tied to a
particular compiler and most often to a particular version, disambiguating
module names can be a daunting task. For example, modules corresponding to
version~1.7.3 of the Open\,MPI library built with different compilers might be named
as follows:
{\small
\begin{alltt}
    \textbf{\% module avail OpenMPI}
    OpenMPI/1.7.3-GCC-4.8.2
    OpenMPI/1.7.3-Intel-14.0\
\end{alltt}
}

The situation becomes even more complicated for scientific software
packages like WRF compiled with a particular compiler and linked
against a particular MPI library, e.g.:
{\small
\begin{alltt}
    \textbf{\% module avail WRF}
    WRF/3.5-GCC-4.8.2-OpenMPI-1.7.3
    WRF/3.5-Intel-14.0-MVAPICH-1.9\
\end{alltt}
}
\noindent
Note that such packages in many cases also depend on a set of mathematical
libraries, such as OpenBLAS+(Sca)LAPACK+FFTW vs. ACML vs. Intel MKL, which complicates matters even further.

A possible solution to this issue is to define so-called \emph{toolchain}
modules, packaging together a compiler, an MPI library, and one or more packages
providing linear algebra and FFT functionality. For example, a \texttt{\small goolf}
toolchain module may combine (i.e., implicitly load modules for) a compatible combination of specific versions of GCC,
Open\,MPI, OpenBLAS, (Sca)LAPACK and FFTW. In this approach, the
first WRF module shown above could instead be installed on top of such a toolchain,
and might take the, slightly shorter, name \texttt{\small WRF/3.5-goolf-1.6.10}.
One downside of using toolchains with terse naming conventions, however, is clear: toolchain names can be cryptic, and the toolchain version generally has no direct relationship to the versions of the encapsulated packages, so users lack top-level insight into 
this information.

One common attempt to manage the potentially overwhelming set of available
module files is to group them in different subdirectories. This makes 
it possible to list the subdirectories separately in the
\texttt{\small\$MODULEPATH}, resulting in clearer, more useful
 module names that are
nicely separated in the output of
\texttt{\small module avail}, for example:
{\small
\begin{alltt}
    \textbf{\% module avail}
    ----- <\emph{<prefix>}/compiler -----
    GCC/4.8.2   Intel/14.0  Clang/3.4
    -------- \emph{<prefix>}/mpi --------
    OpenMPI/1.7.3-GCC-4.8.2
    OpenMPI/1.7.3-Intel-14.0\
\ignore{
    -------- \emph{<prefix>}/apps --------
    WRF/3.5-GCC-4.8.2-OpenMPI-1.7.3
    WRF/3.5-Intel-14.0-OpenMPI-1.7.3\
}
\end{alltt}
}
\noindent
However, this involves picking a single category for each software package to
install, which may become cumbersome with scientific software that crosses multiple
research domains. The toolchain concept also offers the possibility to categorize
modules by toolchain, but if a software package is available for multiple
toolchains, it will show up in multiple sections of the \texttt{\small module
avail} output, which is not desirable.


Although all these approaches aim to improve the overall
organization of the available modules, a typical module listing on an HPC
system can still be overwhelming, as the total number of modules can easily
be in the order of several hundreds. Moreover, none of these approaches prevent (especially novice) users shooting themselves in the
foot by loading modules which are incompatible to each other. While
there are ways to prevent this, they require identifying and explicitly listing
all conflicting modules. For example, the Open\,MPI module may
specify that it is incompatible with modules that load other MPI libraries like
Intel MPI or MVAPICH. However, this means that these conflict specifications have
to be adjusted every time an additional MPI library is installed on the
system, which is clearly a maintenance nightmare for system administrators.

The `flat' module naming scheme discussed here is common, but it places a significant
burden on users. If a user requires only a single application like WRF, picking a
module is fairly straightforward. But when multiple software packages are required
at the same time, e.g., when an application developer is using multiple libraries to
build a parallel application, he or she must pick modules for a
compiler, MPI stack and other required libraries that are compatible with each other.
After loading a mismatched set of modules the user might get lucky: the application might fail to run or die immediately.  However, the
application may also fail in subtle and mysterious ways, and can thus consume a
great deal of time from both users and system staff in trying to resolve the failure.
Section~\ref{sec:hierarchical} outlines another approach to remove this burden
from users and staff.


%\remark{main issues with module files: maintaining consistency (contents of module files, naming),
%putting the burden on users to correctly align things and not run into trouble (avoiding conflicts, etc.)}
%
%
%\remark{module naming scheme should be discussed separately from module files, different concepts,
%making the distinction is important in the paper context}

%\remark{flat naming scheme (most common?),

\subsection{Building and installing scientific software}
\label{sec:installing}

HPC sites around the world use a wide variety of methods and tools to
install scientific software. In this section, we provide a brief
overview of these techniques,
highlighting the associated issues and problems. These observations are
supported by the results of a recent poll concerning these
topics~\cite{ISC14bof}.

\subsubsection{Manual installation}

Commonly, sites rely heavily on the manpower of (a part of)
the user support team, and simply manually install software packages following
the install guides developed internally over time or provided
by the respective software development teams (if the latter are available, current, and
sufficiently detailed).

\subsubsection{Scripting}

Frequently, sites cobble together a collection of scripts to automate the often repetitive and error-prone tasks of
configuring, building and installing the software packages, using any of a variety of scripting languages. Typically, this quickly results in a pile of
loosely coupled, hard-to-maintain scripts, often
understood by just a small fraction or even a single member of the user support
team~\cite{Jim}. On top of this, these
scripts tend to encode site-specific software installation policies, making reuse by other sites difficult (assuming the scripts
are made available to others).

\subsubsection{Package managers}

Yet another approach is to rely on the package managing tools used
by the operating system, e.g., RPMs and \texttt{\small yum} for RedHat-based systems,
\texttt{\small apt-get} and Debian packages for Debian-like systems, Portage for
Gentoo, etc.
Package managers provide adequate support for some aspects of
installing large software stacks, including dependency tracking/resolution, software
updates, uninstalling software, etc. However, they are ill-suited for dealing with
certain peculiarities that come into play when installing scientific software on
HPC systems. These include supporting multiple builds/versions of the same software package
to be installed at the same time, and heavily customized install procedures involving
non-standard techniques beyond the common \texttt{\small configure} --
\texttt{\small make} -- \texttt{\small make install} paradigm. In addition, the package specification formats (e.g.,
\texttt{\small .spec} files for RPMs) tend to provide little support for factoring out
common patterns in install procedures, leading to copy-pasted, hard-to-maintain package specifications.

Several of the larger HPC sites in the world take this approach, since they are able to dedicate large amounts of manpower to
the task of installing scientific software, but this is typically infeasible for
smaller HPC sites. Besides these concerns, the effort spent on
shoe-horning the install procedures of scientific software into package
specifications are unlikely to benefit other HPC sites: 
site-specific installation policies require labor-intensive modifications, and
redistribution of packaged builds is sometimes prohibited for
licensed software.

Examples include LosF~\cite{lmodSC11} developed by the Texas
Advanced Computing Center (TACC) to leverage RPMs developed in-house
(encapsulating both the software itself and the
accompanying module file), the XSEDE Compatible Basic Cluster (XCBC)~\cite{Fischer14}
which provides a collection of RPMs that allow for making an HPC system
`XSEDE-compatible', and CernVM-FS~\cite{cernvm-fs}, a read-only distributed file
system based on HTTP which is optimized to distribute readily built software
applications.

\subsubsection{Custom tools}

Other solutions include custom-made tools for installing scientific software
on HPC systems. We briefly discuss a number of these in
Section~\ref{sec:related_work}. Typically, these tools begin as a collection of scripts and mature as the complexity of the local operation increases; they often reach a point where they may prove useful for other sites. Unfortunately, these projects typically die a
silent death as quickly as they surfaced: the sole original developer is no longer available, the documentation is inadequate, or the
infrastructure is not sufficiently flexible or feature-rich to support the needs of multiple sites. In Section~\ref{sec:easybuild} we discuss in detail one notable
exception, \easybuild{}~\cite{EasyBuildSC12}.

\subsubsection{Creating module files}

It is tempting to deem module files ``simple enough" to create 
manually, and in fact this is common practice. However, this significantly impedes
the goal of maintaining a consistent set of module files. Moreover, the burden of doing this work often falls on a small number of intrepid staff members (if not a single person). This is clearly a major concern
on production systems, where continuity of support is a high priority.

%%???
%%
%%\remark{manual, in-house scripting, 'Jim', little to no collaboration across
%%HPC sites (a few exceptions!)}
%%
%%\markus{Does it make sense to also mention RPM and DEB packages? For a
%%regular software install, the downside is that only one version can be
%%available. But of course packaging systems can be combined with our proposed
%%approach to roll out the software on a bunch of nodes (see TACC).}
%%
%%\kenneth{imho, yes, it makes sense to mention the open issues with traditional
%%packaging systems: only one version/build per software package, usually little
%%support for the mess you run into with scientific software, ...}
%%
%%\remark{issues with using packaging tools like RPM (cfr. TACC): requires tons of manpower,
%%very little flexibility for site-specific modifications, little collaboration between sites thus
%%lots of duplicate effort}

\subsection{Lack of collaboration}
\label{sec:traditional_lack}

Even though these problems are well recognized, there is an abundant lack
of available tools and practices for addressing them.
Moreover, there has been very little collaboration between HPC sites on these
issues, despite the significant burden they present for support staff. This is especially true at smaller sites. Even though HPC sites around the world all face these problems, the duplication of effort and associated labor cost is alarming. There is a tremendous opportunity
here: we all stand to benefit from collaborative initiatives that
leverage the
immensely valuable expertise available across HPC sites worldwide.

In the remainder of this paper we seek to help resolve these problems
by describing a modern, robust, and flexible alternative to these
traditional approaches.


%???
%\remark{manual creating of module files (consistency issues), repetitive \&
%error-prone work of installing scientific software, \ldots}
